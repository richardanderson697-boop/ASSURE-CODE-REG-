# main.py - Updated with PostgreSQL Integration

import os
import asyncio
import hashlib
import uuid
from datetime import datetime
from typing import List, Dict, Optional

from fastapi import FastAPI, HTTPException, BackgroundTasks, Depends, Header
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, HttpUrl
from dotenv import load_dotenv
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

# Database imports
from database import (
    get_db, init_db,
    User, Organization, ScrapeJob, RegulationDocument, DocumentChunk,
    ComplianceAnalysis, ApiKey, AuditLog,
    JobStatus, generate_api_key, verify_api_key
)

# Web Scraping
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import robotexclusionrulesparser as rerp

# RAG Components
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.docstore.document import Document
from langchain.prompts import PromptTemplate

# PDF Processing
from reportlab.lib.pagesizes import letter
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch

load_dotenv()

app = FastAPI(title="Regulatory Compliance RAG Scraper - PostgreSQL Edition")

# CORS configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================================================
# STARTUP/SHUTDOWN EVENTS
# ============================================================================

@app.on_event("startup")
async def startup_event():
    """Initialize database on startup"""
    await init_db()
    print("âœ… Database initialized")

# ============================================================================
# AUTHENTICATION & AUTHORIZATION
# ============================================================================

async def get_current_user(
    authorization: str = Header(None),
    db: AsyncSession = Depends(get_db)
) -> User:
    """Verify API key and return user"""
    
    if not authorization:
        raise HTTPException(status_code=401, detail="Authorization header required")
    
    if not authorization.startswith("Bearer "):
        raise HTTPException(status_code=401, detail="Invalid authorization format")
    
    api_key = authorization.replace("Bearer ", "")
    
    # Hash the provided key
    key_hash = hashlib.sha512(api_key.encode()).hexdigest()
    
    # Find API key in database
    result = await db.execute(
        select(ApiKey).where(ApiKey.key_hash == key_hash, ApiKey.is_active == True)
    )
    api_key_obj = result.scalar_one_or_none()
    
    if not api_key_obj:
        raise HTTPException(status_code=401, detail="Invalid API key")
    
    # Update last used
    api_key_obj.last_used = datetime.utcnow()
    api_key_obj.usage_count += 1
    await db.commit()
    
    # Get organization's first active user (simplified - enhance for multi-user orgs)
    result = await db.execute(
        select(User).where(
            User.organization_id == api_key_obj.organization_id,
            User.is_active == True
        ).limit(1)
    )
    user = result.scalar_one_or_none()
    
    if not user:
        raise HTTPException(status_code=401, detail="No active user found")
    
    return user

async def log_audit_event(
    db: AsyncSession,
    user_id: Optional[str],
    action: str,
    resource_type: str = None,
    resource_id: str = None,
    details: dict = None,
    success: bool = True,
    error_message: str = None,
    ip_address: str = None
):
    """Create an audit log entry"""
    
    audit_log = AuditLog(
        id=str(uuid.uuid4()),
        user_id=user_id,
        action=action,
        resource_type=resource_type,
        resource_id=resource_id,
        details=details,
        success=success,
        error_message=error_message,
        ip_address=ip_address,
        timestamp=datetime.utcnow()
    )
    
    db.add(audit_log)
    await db.commit()

# ============================================================================
# DATA MODELS (Pydantic)
# ============================================================================

class ScrapeRequest(BaseModel):
    url: HttpUrl
    jurisdiction: str = "US"
    category: str = "finance"
    max_pages: int = 10
    respect_robots_txt: bool = True
    rate_limit_ms: int = 2000

class AnalysisQuery(BaseModel):
    job_id: str
    question: str
    context_depth: int = 5

class JobResponse(BaseModel):
    job_id: str
    status: str
    progress: int
    url: str
    started_at: str
    completed_at: Optional[str]
    documents_scraped: int
    error: Optional[str]

class CreateOrganizationRequest(BaseModel):
    name: str
    subscription_tier: str = "professional"

class CreateApiKeyRequest(BaseModel):
    name: str
    rate_limit_per_minute: int = 60

# ============================================================================
# ETHICAL SCRAPING ENGINE (Same as before)
# ============================================================================

class EthicalScraper:
    def __init__(self, rate_limit_ms: int = 2000):
        self.rate_limit_ms = rate_limit_ms
        self.user_agent = "RegulatoryComplianceBot/1.0 (+https://compliance-scraper.com/bot.html)"
        self.robots_cache = {}
    
    async def check_robots_txt(self, url: str) -> bool:
        """Check if URL is allowed by robots.txt"""
        from urllib.parse import urlparse
        parsed = urlparse(url)
        base_url = f"{parsed.scheme}://{parsed.netloc}"
        
        if base_url not in self.robots_cache:
            robots_url = f"{base_url}/robots.txt"
            try:
                rp = rerp.RobotExclusionRulesParser()
                rp.user_agent = self.user_agent
                await asyncio.sleep(1)
                rp.fetch(robots_url)
                self.robots_cache[base_url] = rp
            except Exception as e:
                print(f"Could not fetch robots.txt: {e}")
                return True
        
        rp = self.robots_cache[base_url]
        return rp.is_allowed(self.user_agent, url)
    
    async def scrape_page(self, url: str, respect_robots: bool = True) -> Dict:
        """Scrape a single page with ethical constraints"""
        
        if respect_robots and not await self.check_robots_txt(url):
            raise HTTPException(403, f"URL disallowed by robots.txt: {url}")
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(user_agent=self.user_agent)
            page = await context.new_page()
            
            try:
                await page.goto(str(url), wait_until='networkidle', timeout=30000)
                await asyncio.sleep(self.rate_limit_ms / 1000)
                
                content = await page.content()
                title = await page.title()
                
                soup = BeautifulSoup(content, 'html.parser')
                for script in soup(["script", "style", "nav", "footer"]):
                    script.decompose()
                
                text = soup.get_text(separator='\n', strip=True)
                
                metadata = {
                    'url': str(url),
                    'title': title,
                    'scraped_at': datetime.utcnow().isoformat(),
                }
                
                date_meta = soup.find('meta', {'name': 'date'}) or \
                           soup.find('meta', {'property': 'article:published_time'})
                if date_meta:
                    metadata['publication_date'] = date_meta.get('content')
                
                await browser.close()
                
                return {
                    'text': text,
                    'metadata': metadata,
                    'html': content
                }
                
            except Exception as e:
                await browser.close()
                raise HTTPException(500, f"Scraping error: {str(e)}")

# ============================================================================
# RAG PROCESSING ENGINE
# ============================================================================

class RAGProcessor:
    def __init__(self):
        self.embeddings = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))
        self.llm = ChatOpenAI(model_name="gpt-4", temperature=0, 
                             openai_api_key=os.getenv("OPENAI_API_KEY"))
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000, chunk_overlap=200, length_function=len
        )
    
    async def process_and_store_document(
        self,
        db: AsyncSession,
        scrape_job_id: str,
        scraped_data: Dict
    ) -> RegulationDocument:
        """Process scraped data and store in database"""
        
        text = scraped_data['text']
        metadata = scraped_data['metadata']
        
        # Create document hash
        text_hash = hashlib.sha256(text.encode()).hexdigest()
        
        # Check if document already exists
        result = await db.execute(
            select(RegulationDocument).where(RegulationDocument.text_hash == text_hash)
        )
        existing_doc = result.scalar_one_or_none()
        
        if existing_doc:
            return existing_doc
        
        # Create new document
        doc = RegulationDocument(
            id=str(uuid.uuid4()),
            scrape_job_id=scrape_job_id,
            title=metadata.get('title', 'Untitled'),
            source_url=metadata['url'],
            raw_text=text,
            raw_html=scraped_data.get('html'),
            text_hash=text_hash,
            word_count=len(text.split()),
            embedding_model="text-embedding-ada-002",
            scraped_at=datetime.utcnow()
        )
        
        db.add(doc)
        await db.commit()
        await db.refresh(doc)
        
        return doc
    
    async def create_embeddings_and_chunks(
        self,
        db: AsyncSession,
        document: RegulationDocument,
        job_id: str
    ):
        """Create vector embeddings and store chunks"""
        
        # Split text into chunks
        langchain_doc = Document(
            page_content=document.raw_text,
            metadata={'source': document.source_url}
        )
        chunks = self.text_splitter.split_documents([langchain_doc])
        
        # Create vector store
        vector_store = Chroma(
            collection_name=f"job_{job_id}",
            embedding_function=self.embeddings,
            persist_directory=f"./chroma_db/{job_id}"
        )
        
        # Add chunks to vector store and database
        chunk_texts = [chunk.page_content for chunk in chunks]
        vector_ids = vector_store.add_texts(chunk_texts)
        
        # Store chunk references in database
        for idx, (chunk, vector_id) in enumerate(zip(chunks, vector_ids)):
            db_chunk = DocumentChunk(
                id=str(uuid.uuid4()),
                document_id=document.id,
                chunk_index=idx,
                content=chunk.page_content,
                vector_id=vector_id
            )
            db.add(db_chunk)
        
        # Update document with chunk count
        document.chunk_count = len(chunks)
        
        await db.commit()
        
        return vector_store
    
    async def analyze_compliance(
        self,
        db: AsyncSession,
        job_id: str,
        jurisdiction: str,
        category: str
    ) -> ComplianceAnalysis:
        """Perform semantic analysis and store results"""
        
        # Load vector store
        vector_store = Chroma(
            collection_name=f"job_{job_id}",
            embedding_function=self.embeddings,
            persist_directory=f"./chroma_db/{job_id}"
        )
        
        # Create QA chain
        template = """You are an expert regulatory compliance analyst. 
        Use the following context to answer questions about regulations.

        Context: {context}
        Question: {question}

        Provide detailed answers with:
        1. Specific requirements and obligations
        2. Key dates and deadlines
        3. Affected entities
        4. Penalties for non-compliance
        5. Source references

        Answer:"""
        
        QA_PROMPT = PromptTemplate(template=template, input_variables=["context", "question"])
        
        qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=vector_store.as_retriever(search_kwargs={"k": 5}),
            chain_type_kwargs={"prompt": QA_PROMPT},
            return_source_documents=True
        )
        
        # Analyze with key questions
        questions = [
            f"What are the new compliance requirements for {category} in {jurisdiction}?",
            "What are the key effective dates and deadlines?",
            "What are the penalties for non-compliance?",
            "Which entities are affected?",
            "What actions must companies take?"
        ]
        
        findings = []
        total_tokens = 0
        
        for question in questions:
            result = qa_chain({"query": question})
            findings.append({
                'question': question,
                'answer': result['result'],
                'sources': [doc.metadata.get('source', 'Unknown') 
                           for doc in result['source_documents']]
            })
            total_tokens += 500  # Approximate
        
        # Store analysis
        analysis = ComplianceAnalysis(
            id=str(uuid.uuid4()),
            scrape_job_id=job_id,
            findings={
                'jurisdiction': jurisdiction,
                'category': category,
                'analyzed_at': datetime.utcnow().isoformat(),
                'findings': findings
            },
            llm_model="gpt-4",
            total_tokens_used=total_tokens,
            analyzed_at=datetime.utcnow()
        )
        
        db.add(analysis)
        await db.commit()
        await db.refresh(analysis)
        
        return analysis

# ============================================================================
# BACKGROUND JOB EXECUTION
# ============================================================================

async def execute_scrape_job(job_id: str, request: ScrapeRequest):
    """Execute scraping and RAG processing"""
    
    from database import async_session_maker
    
    async with async_session_maker() as db:
        try:
            # Get job
            result = await db.execute(select(ScrapeJob).where(ScrapeJob.id == job_id))
            job = result.scalar_one()
            
            # Update status
            job.status = JobStatus.SCRAPING
            job.progress = 10
            await db.commit()
            
            # Scrape
            scraper = EthicalScraper(rate_limit_ms=request.rate_limit_ms)
            scraped_data = await scraper.scrape_page(
                str(request.url),
                respect_robots=request.respect_robots_txt
            )
            
            job.progress = 40
            job.documents_scraped = 1
            await db.commit()
            
            # Process document
            job.status = JobStatus.PROCESSING
            await db.commit()
            
            rag = RAGProcessor()
            document = await rag.process_and_store_document(db, job_id, scraped_data)
            
            job.progress = 60
            await db.commit()
            
            # Create embeddings
            await rag.create_embeddings_and_chunks(db, document, job_id)
            
            job.progress = 80
            job.status = JobStatus.ANALYZING
            await db.commit()
            
            # Analyze
            await rag.analyze_compliance(db, job_id, request.jurisdiction, request.category)
            
            # Complete
            job.status = JobStatus.COMPLETED
            job.progress = 100
            job.completed_at = datetime.utcnow()
            await db.commit()
            
        except Exception as e:
            job.status = JobStatus.FAILED
            job.error_message = str(e)
            await db.commit()

# ============================================================================
# API ENDPOINTS
# ============================================================================

@app.post("/api/v1/organizations")
async def create_organization(
    request: CreateOrganizationRequest,
    db: AsyncSession = Depends(get_db)
):
    """Create a new organization"""
    
    org = Organization(
        id=f"org_{uuid.uuid4().hex[:16]}",
        name=request.name,
        subscription_tier=request.subscription_tier
    )
    
    db.add(org)
    await db.commit()
    await db.refresh(org)
    
    return {
        'organization_id': org.id,
        'name': org.name,
        'status': org.subscription_status.value
    }

@app.post("/api/v1/organizations/{org_id}/api-keys")
async def create_api_key_for_org(
    org_id: str,
    request: CreateApiKeyRequest,
    db: AsyncSession = Depends(get_db)
):
    """Create API key for an organization"""
    
    # Verify organization exists
    result = await db.execute(select(Organization).where(Organization.id == org_id))
    org = result.scalar_one_or_none()
    
    if not org:
        raise HTTPException(404, "Organization not found")
    
    # Generate key
    full_key, key_hash, key_prefix = generate_api_key()
    
    # Store in database
    api_key = ApiKey(
        id=str(uuid.uuid4()),
        organization_id=org_id,
        name=request.name,
        key_hash=key_hash,
        key_prefix=key_prefix,
        rate_limit_per_minute=request.rate_limit_per_minute
    )
    
    db.add(api_key)
    await db.commit()
    
    return {
        'api_key_id': api_key.id,
        'api_key': full_key,  # Only shown once!
        'name': api_key.name,
        'created_at': api_key.created_at.isoformat()
    }

@app.post("/api/v1/scrape", response_model=JobResponse)
async def create_scrape_job(
    request: ScrapeRequest,
    background_tasks: BackgroundTasks,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Create a new scraping job"""
    
    job_id = str(uuid.uuid4())
    
    job = ScrapeJob(
        id=job_id,
        user_id=current_user.id,
        organization_id=current_user.organization_id,
        url=str(request.url),
        jurisdiction=request.jurisdiction,
        category=request.category,
        max_pages=request.max_pages,
        respect_robots_txt=request.respect_robots_txt,
        rate_limit_ms=request.rate_limit_ms,
        status=JobStatus.QUEUED,
        progress=0,
        started_at=datetime.utcnow()
    )
    
    db.add(job)
    await db.commit()
    
    # Log audit event
    await log_audit_event(
        db, current_user.id, "scrape_job_created",
        "scrape_job", job_id,
        {'url': str(request.url), 'jurisdiction': request.jurisdiction}
    )
    
    # Add background task
    background_tasks.add_task(execute_scrape_job, job_id, request)
    
    return JobResponse(
        job_id=job.id,
        status=job.status.value,
        progress=job.progress,
        url=job.url,
        started_at=job.started_at.isoformat(),
        completed_at=None,
        documents_scraped=job.documents_scraped,
        error=None
    )

@app.get("/api/v1/jobs/{job_id}", response_model=JobResponse)
async def get_job_status(
    job_id: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Get job status"""
    
    result = await db.execute(
        select(ScrapeJob).where(
            ScrapeJob.id == job_id,
            ScrapeJob.organization_id == current_user.organization_id
        )
    )
    job = result.scalar_one_or_none()
    
    if not job:
        raise HTTPException(404, "Job not found")
    
    return JobResponse(
        job_id=job.id,
        status=job.status.value,
        progress=job.progress,
        url=job.url,
        started_at=job.started_at.isoformat(),
        completed_at=job.completed_at.isoformat() if job.completed_at else None,
        documents_scraped=job.documents_scraped,
        error=job.error_message
    )

@app.post("/api/v1/analyze")
async def analyze_with_rag(
    query: AnalysisQuery,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Ask questions using RAG"""
    
    # Verify job belongs to user's org
    result = await db.execute(
        select(ScrapeJob).where(
            ScrapeJob.id == query.job_id,
            ScrapeJob.organization_id == current_user.organization_id
        )
    )
    job = result.scalar_one_or_none()
    
    if not job:
        raise HTTPException(404, "Job not found")
    
    if job.status != JobStatus.COMPLETED:
        raise HTTPException(400, "Job not completed yet")
    
    # Load vector store and query
    rag = RAGProcessor()
    vector_store = Chroma(
        collection_name=f"job_{query.job_id}",
        embedding_function=rag.embeddings,
        persist_directory=f"./chroma_db/{query.job_id}"
    )
    
    # Create QA chain
    template = """Context: {context}\nQuestion: {question}\nAnswer:"""
    QA_PROMPT = PromptTemplate(template=template, input_variables=["context", "question"])
    
    qa_chain = RetrievalQA.from_chain_type(
        llm=rag.llm,
        retriever=vector_store.as_retriever(search_kwargs={"k": query.context_depth}),
        chain_type_kwargs={"prompt": QA_PROMPT},
        return_source_documents=True
    )
    
    result = qa_chain({"query": query.question})
    
    # Log query
    await log_audit_event(
        db, current_user.id, "rag_query",
        "scrape_job", query.job_id,
        {'question': query.question}
    )
    
    return {
        'job_id': query.job_id,
        'question': query.question,
        'answer': result['result'],
        'sources': [{'url': doc.metadata.get('source')} 
                   for doc in result['source_documents']]
    }

@app.get("/api/v1/export/json/{job_id}")
async def export_json(
    job_id: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Export analysis as JSON"""
    
    # Get analysis
    result = await db.execute(
        select(ComplianceAnalysis)
        .join(ScrapeJob)
        .where(
            ScrapeJob.id == job_id,
            ScrapeJob.organization_id == current_user.organization_id
        )
    )
    analysis = result.scalar_one_or_none()
    
    if not analysis:
        raise HTTPException(404, "Analysis not found")
    
    await log_audit_event(
        db, current_user.id, "export_json",
        "compliance_analysis", analysis.id
    )
    
    return {
        'job_id': job_id,
        'analysis': analysis.findings,
        'exported_at': datetime.utcnow().isoformat()
    }

@app.get("/health")
async def health_check(db: AsyncSession = Depends(get_db)):
    """Health check"""
    
    # Test database connection
    await db.execute(select(1))
    
    return {
        'status': 'healthy',
        'database': 'connected',
        'timestamp': datetime.utcnow().isoformat()
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)